{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0998171-028d-4dd6-90d3-c8020113ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, init\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ShareLinearFull(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, latent_size=3):\n",
    "        super(ShareLinearFull, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.latent_params = Parameter(torch.Tensor(latent_size))\n",
    "        self.warp = Parameter(torch.Tensor(in_features * out_features, latent_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def get_weight(self):\n",
    "        return (self.warp @ self.latent_params).view(self.out_features, self.in_features)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init._no_grad_normal_(self.warp, 0, 0.01)\n",
    "        init._no_grad_normal_(self.latent_params, 0, 1 / self.out_features)\n",
    "        if self.bias is not None:\n",
    "            weight = self.get_weight()\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight = self.get_weight()\n",
    "        return F.linear(x, weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c10b51-87bb-48da-8244-25f7e7e1247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features, bias, latent_size = 70, 68, False, 3\n",
    "mod = ShareLinearFull(in_features, out_features, bias, latent_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e8339-2b24-4113-964d-f2d14c6ac346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Opt class from https://github.com/AllanYangZhou/metalearning-symmetries/blob/master/inner_optimizers.py\n",
    "import collections\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "\n",
    "def is_warp_layer(name):\n",
    "    return \"warp\" in name\n",
    "\n",
    "\n",
    "NAME_TO_INNER_OPT_CLS = {\n",
    "    \"maml\": SGD,\n",
    "    \"maml_adam\": Adam,\n",
    "}\n",
    "\n",
    "\n",
    "# TODO(allanz): Refactor into a module (or several), similar to ebn in higher/examples.\n",
    "class InnerOptBuilder:\n",
    "    def __init__(self, network, device, opt_name, init_lr, init_mode, lr_mode, ext_metaparams=None):\n",
    "        self.network = network\n",
    "        self.opt_name = opt_name\n",
    "        self.init_lr = init_lr\n",
    "        self.init_mode = init_mode\n",
    "        self.lr_mode = lr_mode\n",
    "        # metaparams that are not neural network params (e.g., learned lrs)\n",
    "        if ext_metaparams:\n",
    "            self.ext_metaparams = ext_metaparams\n",
    "        else:\n",
    "            self.ext_metaparams = self.make_ext_metaparams(device)\n",
    "        self.inner_opt_cls = NAME_TO_INNER_OPT_CLS[opt_name]\n",
    "        self.inner_opt = NAME_TO_INNER_OPT_CLS[opt_name](self.param_groups, lr=self.init_lr)\n",
    "\n",
    "    def make_ext_metaparams(self, device):\n",
    "        ext_metaparams = {}\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if is_warp_layer(name) or not param.requires_grad:\n",
    "                # Ignore symmetry params in the inner loop.\n",
    "                continue\n",
    "            if self.lr_mode == \"per_layer\":\n",
    "                inner_lr = torch.tensor(self.init_lr).to(device)\n",
    "                inner_lr.requires_grad = True\n",
    "                ext_metaparams[f\"{name}_lr\"] = inner_lr\n",
    "            elif self.lr_mode == \"per_param\":\n",
    "                inner_lr = self.init_lr * torch.ones_like(param).to(device)\n",
    "                inner_lr.requires_grad = True\n",
    "                ext_metaparams[f\"{name}_lr\"] = inner_lr\n",
    "            elif self.lr_mode == \"fixed\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized lr_mode: {self.lr_mode}\")\n",
    "        return ext_metaparams\n",
    "\n",
    "    @property\n",
    "    def metaparams(self):\n",
    "        metaparams = {}\n",
    "        metaparams.update(self.ext_metaparams)\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if is_warp_layer(name) or self.init_mode == \"learned\":\n",
    "                metaparams[name] = param\n",
    "        return metaparams\n",
    "\n",
    "    @property\n",
    "    def param_groups(self):\n",
    "        param_groups = []\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if is_warp_layer(name) or not param.requires_grad:\n",
    "                # Ignore symmetry params in the inner loop.\n",
    "                continue\n",
    "            param_groups.append({\"params\": param})\n",
    "        return param_groups\n",
    "\n",
    "    @property\n",
    "    def overrides(self):\n",
    "        overrides = collections.defaultdict(list)\n",
    "        for name, param in self.network.named_parameters():\n",
    "            if is_warp_layer(name) or not param.requires_grad:\n",
    "                # Ignore symmetry params in the inner loop.\n",
    "                continue\n",
    "            if self.lr_mode == \"per_layer\":\n",
    "                overrides[\"lr\"].append(self.ext_metaparams[f\"{name}_lr\"])\n",
    "            elif self.lr_mode == \"per_param\":\n",
    "                overrides[\"lr\"].append(self.ext_metaparams[f\"{name}_lr\"])\n",
    "            elif self.lr_mode == \"fixed\":\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized lr_mode: {self.lr_mode}\")\n",
    "        return overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596c9ee-723c-47c8-baec-696d5a3fe5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test loops from https://github.com/AllanYangZhou/metalearning-symmetries/blob/master/train_synthetic.py\n",
    "import scipy.stats as st\n",
    "\n",
    "def train(step_idx, data, net, inner_opt_builder, meta_opt, n_inner_iter):\n",
    "    \"\"\"Main meta-training step.\"\"\"\n",
    "    x_spt, y_spt, x_qry, y_qry = data\n",
    "    task_num = x_spt.size()[0]\n",
    "    querysz = x_qry.size(1)\n",
    "\n",
    "    inner_opt = inner_opt_builder.inner_opt\n",
    "\n",
    "    qry_losses = []\n",
    "    meta_opt.zero_grad()\n",
    "    for i in range(task_num):\n",
    "        with higher.innerloop_ctx(\n",
    "            net,\n",
    "            inner_opt,\n",
    "            copy_initial_weights=False,\n",
    "            override=inner_opt_builder.overrides,\n",
    "        ) as (\n",
    "            fnet,\n",
    "            diffopt,\n",
    "        ):\n",
    "            for _ in range(n_inner_iter):\n",
    "                spt_pred = fnet(x_spt[i])\n",
    "                spt_loss = F.mse_loss(spt_pred, y_spt[i])\n",
    "                diffopt.step(spt_loss)\n",
    "            qry_pred = fnet(x_qry[i])\n",
    "            qry_loss = F.mse_loss(qry_pred, y_qry[i])\n",
    "            qry_losses.append(qry_loss.detach().cpu().numpy())\n",
    "            qry_loss.backward()\n",
    "    #metrics = {\"train_loss\": np.mean(qry_losses)}\n",
    "    meta_opt.step()\n",
    "\n",
    "\n",
    "def test(step_idx, data, net, inner_opt_builder, n_inner_iter):\n",
    "    \"\"\"Main meta-training step.\"\"\"\n",
    "    x_spt, y_spt, x_qry, y_qry = data\n",
    "    task_num = x_spt.size()[0]\n",
    "    querysz = x_qry.size(1)\n",
    "\n",
    "    inner_opt = inner_opt_builder.inner_opt\n",
    "\n",
    "    qry_losses = []\n",
    "    for i in range(task_num):\n",
    "        with higher.innerloop_ctx(\n",
    "            net, inner_opt, track_higher_grads=False, override=inner_opt_builder.overrides,\n",
    "        ) as (\n",
    "            fnet,\n",
    "            diffopt,\n",
    "        ):\n",
    "            for _ in range(n_inner_iter):\n",
    "                spt_pred = fnet(x_spt[i])\n",
    "                spt_loss = F.mse_loss(spt_pred, y_spt[i])\n",
    "                diffopt.step(spt_loss)\n",
    "            qry_pred = fnet(x_qry[i])\n",
    "            qry_loss = F.mse_loss(qry_pred, y_qry[i])\n",
    "            qry_losses.append(qry_loss.detach().cpu().numpy())\n",
    "    avg_qry_loss = np.mean(qry_losses)\n",
    "    _low, high = st.t.interval(\n",
    "        0.95, len(qry_losses) - 1, loc=avg_qry_loss, scale=st.sem(qry_losses)\n",
    "    )\n",
    "    #test_metrics = {\"test_loss\": avg_qry_loss, \"test_err\": high - avg_qry_loss}\n",
    "    return avg_qry_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46dd4ef-a254-4537-9a51-f8f3abda06b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
